import os
import time
import re
import requests

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

# ====== é…ç½®å‚æ•° ======
URL_LIST_FILE = 'E:\\BROWSER\\urls.txt'
SAVE_ROOT = 'E:\\BROWSER\\dl'
CHROMEDRIVER_PATH = 'E:\\ä¸‹è½½\\picture\\chromedriver.exe'
SCROLL_PAUSE_TIME = 3
MAX_IDLE_ROUNDS = 9

# ====== Selenium åˆå§‹åŒ– ======
chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--disable-gpu')
chrome_options.add_argument('--log-level=3')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

service = Service(CHROMEDRIVER_PATH)
driver = webdriver.Chrome(service=service, options=chrome_options)

# ====== å·¥å…·å‡½æ•° ======

def sanitize_filename(name):
    """å»æ‰ Windows/Unix ä¸å…è®¸çš„å­—ç¬¦"""
    return re.sub(r'[\\/:*?"<>|]', '_', name)


def scroll_to_load_all_images(driver, pause_time=2, max_idle_rounds=5):
    """
    æ— é™ä¸‹æ‹‰åŠ è½½ï¼Œç›´åˆ°å›¾ç‰‡æ•°è¿ç»­å‡ è½®æ²¡æœ‰å¢åŠ 
    """
    previous_count = 0
    idle_rounds = 0

    while idle_rounds < max_idle_rounds:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(pause_time)

        images = driver.find_elements(By.TAG_NAME, 'img')
        current_count = len(images)
        print(f"  ğŸ“· å½“å‰å·²åŠ è½½å›¾ç‰‡æ•°: {current_count}")

        if current_count <= previous_count:
            idle_rounds += 1
            print(f"  âš ï¸ æ²¡æœ‰æ–°å›¾ç‰‡ï¼ˆç©ºè½¬ {idle_rounds}/{max_idle_rounds}ï¼‰")
        else:
            idle_rounds = 0

        previous_count = current_count

    print(f"âœ… æ»šåŠ¨åŠ è½½å®Œæˆï¼Œæ€»å›¾ç‰‡æ•°: {previous_count}")


def extract_image_urls(driver):
    images = driver.find_elements(By.TAG_NAME, 'img')
    urls = []
    for img in images:
        src = img.get_attribute('src')
        if src and src.startswith('http'):
            urls.append(src)
    return urls


def download_images(img_urls, save_folder):
    if not os.path.exists(save_folder):
        os.makedirs(save_folder)

    # å…ˆæ•°ä¸€æ•°ç°æœ‰å›¾ç‰‡æ•°
    existing_files = [f for f in os.listdir(save_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.webp'))]
    existing_count = len(existing_files)
    print(f"âœ… ç›®æ ‡æ–‡ä»¶å¤¹å·²æœ‰ {existing_count} å¼ å›¾ç‰‡ï¼Œå°†ä» {existing_count + 1} å¼€å§‹å‘½å")

    # ç»§ç»­ä¸‹è½½å‰©ä¸‹çš„
    for idx, url in enumerate(img_urls[existing_count:], start=existing_count + 1):
        filename = f"{idx:03}.jpg"
        filepath = os.path.join(save_folder, filename)
        try:
            response = requests.get(url, timeout=15)
            if response.status_code == 200:
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                print(f"    âœ”ï¸ å·²ä¿å­˜: {filename}")
            else:
                print(f"    âš ï¸ ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç  {response.status_code}: {url}")
        except Exception as e:
            print(f"    âš ï¸ é”™è¯¯: {e}")



def process_url(url):
    print(f"\n=== å¤„ç†: {url} ===")
    driver.get(url)
    time.sleep(3)

    # æ— é™æ»šåŠ¨åŠ è½½
    scroll_to_load_all_images(driver, pause_time=SCROLL_PAUSE_TIME, max_idle_rounds=MAX_IDLE_ROUNDS)

    # è·å–ç½‘é¡µæ ‡é¢˜ä½œä¸ºå­æ–‡ä»¶å¤¹å
    title = driver.title.strip()
    folder_name = sanitize_filename(title)
    sub_folder = os.path.join(SAVE_ROOT, folder_name)

    print(f"âœ… ç½‘é¡µæ ‡é¢˜: {title}")

    # æå–å›¾ç‰‡é“¾æ¥
    img_urls = extract_image_urls(driver)
    print(f"âœ… å…±æ‰¾åˆ°å›¾ç‰‡: {len(img_urls)} å¼ ")

    if not img_urls:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œè·³è¿‡ã€‚")
        return

    # ä¸‹è½½
    download_images(img_urls, sub_folder)
    print(f"âœ… å·²å…¨éƒ¨ä¿å­˜åˆ°: {sub_folder}")


def main():
    # ç¡®ä¿æ€»ä¿å­˜æ–‡ä»¶å¤¹å­˜åœ¨
    if not os.path.exists(SAVE_ROOT):
        os.makedirs(SAVE_ROOT)

    # è¯»å–URLåˆ—è¡¨
    if not os.path.isfile(URL_LIST_FILE):
        print(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶: {URL_LIST_FILE}")
        return

    with open(URL_LIST_FILE, 'r', encoding='utf-8') as f:
        urls = [line.strip() for line in f if line.strip()]

    if not urls:
        print("âŒ æ–‡ä»¶é‡Œæ²¡æœ‰ä»»ä½•ç½‘å€")
        return

    print(f"âœ… è¯»å–åˆ° {len(urls)} ä¸ªç½‘å€ã€‚")

    # é€ä¸ªå¤„ç†
    for url in urls:
        try:
            process_url(url)
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {url}\né”™è¯¯: {e}")

    driver.quit()
    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆã€‚")


if __name__ == '__main__':
    main()
